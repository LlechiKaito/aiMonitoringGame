# Gemini Flash 2.0 + Flash Attention 2.0 çµ±åˆã‚¬ã‚¤ãƒ‰

## ğŸš€ æ¦‚è¦

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ã€**Gemini Flash 2.0**ã¨**Flash Attention 2.0**ã‚’çµ±åˆã—ãŸé«˜é€Ÿã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚AI Monitoring Game Backendã«æœ€é©åŒ–ã•ã‚Œã¦ãŠã‚Šã€ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ï¼š

- **Gemini Flash 2.0**: ã‚¯ã‚¨ãƒªã®æ‹¡å¼µãƒ»æ”¹å–„
- **Flash Attention 2.0**: é«˜é€Ÿãªæ³¨æ„æ©Ÿæ§‹ã«ã‚ˆã‚‹é¡ä¼¼åº¦è¨ˆç®—
- **å¤šè¨€èªå¯¾å¿œ**: æ—¥æœ¬èªã‚’å«ã‚€50è¨€èªå¯¾å¿œ
- **é«˜æ€§èƒ½**: GPUæœ€é©åŒ–ã«ã‚ˆã‚‹é«˜é€Ÿå‡¦ç†

## ğŸ“‹ ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶

### æœ€å°è¦ä»¶
- **Python**: 3.9ä»¥ä¸Š
- **ãƒ¡ãƒ¢ãƒª**: 4GBä»¥ä¸Š
- **ãƒ‡ã‚£ã‚¹ã‚¯**: 10GBä»¥ä¸Š

### æ¨å¥¨è¦ä»¶
- **Python**: 3.11ä»¥ä¸Š
- **ãƒ¡ãƒ¢ãƒª**: 8GBä»¥ä¸Š
- **GPU**: NVIDIA GPUï¼ˆFlash Attentionç”¨ï¼‰
- **CUDA**: 12.0ä»¥ä¸Š
- **OS**: Linuxï¼ˆæ¨å¥¨ï¼‰

### Flash Attention 2.0 è¦ä»¶
- **OS**: Linuxï¼ˆæ¨å¥¨ï¼‰
- **GPU**: NVIDIA Ampere/Ada/Hopperä¸–ä»£
- **CUDA**: 12.0ä»¥ä¸Š
- **ãƒ¡ãƒ¢ãƒª**: 8GBä»¥ä¸Šã®GPUãƒ¡ãƒ¢ãƒª

## ğŸ”§ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

### ğŸš€ ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆæ¨å¥¨ï¼‰

```bash
# ä»®æƒ³ç’°å¢ƒä½œæˆ
python -m venv venv

# ä»®æƒ³ç’°å¢ƒã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ãƒˆ
source venv/bin/activate  # macOS/Linux
# venv\Scripts\activate   # Windows

# åŸºæœ¬ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install -r requirements.txt

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆï¼ˆAPI Keyè¨­å®šï¼‰
echo "GEMINI_API_KEY=your-actual-api-key-here" > .env

# å®Ÿè¡Œ
python gemini_flash2_sample.py
```

**æ³¨æ„**: `your-actual-api-key-here`ã‚’å®Ÿéš›ã®Gemini API Keyã«ç½®ãæ›ãˆã¦ãã ã•ã„ã€‚

### ğŸ–¥ï¸ ç’°å¢ƒåˆ¥ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

#### macOSï¼ˆApple Silicon: M1/M2/M3ï¼‰
```bash
# PyTorchã®MPSå¯¾å¿œã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install torch torchvision torchaudio

# ãã®ä»–ã®ä¾å­˜é–¢ä¿‚
pip install -r requirements.txt

# Flash Attentionã¯éå¯¾å¿œï¼ˆè‡ªå‹•çš„ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
python gemini_flash2_sample.py
```

**çµæœ**: 
- âœ… å‹•ä½œã—ã¾ã™
- âš ï¸ Flash Attention 2.0 æœªå¯¾å¿œï¼ˆCPUã§ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—ï¼‰
- ğŸ§  Gemini Flash 2.0 å¯¾å¿œ

#### Windows
```cmd
REM ä»®æƒ³ç’°å¢ƒä½œæˆ
python -m venv venv
venv\Scripts\activate

REM PyTorchã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

REM ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install -r requirements.txt

REM å®Ÿè¡Œ
python gemini_flash2_sample.py
```

**çµæœ**:
- âœ… å‹•ä½œã—ã¾ã™
- âš ï¸ Flash Attention 2.0 å®Ÿé¨“çš„ã‚µãƒãƒ¼ãƒˆ
- ğŸ§  Gemini Flash 2.0 å¯¾å¿œ

#### Linux + NVIDIA GPUï¼ˆæ¨å¥¨ï¼‰
```bash
# CUDAç’°å¢ƒç¢ºèª
nvidia-smi

# PyTorch CUDAç‰ˆã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# åŸºæœ¬ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install -r requirements.txt

# Flash Attention ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆé«˜é€ŸåŒ–ï¼‰
pip install flash-attn --no-build-isolation

# GPUæœ€é©åŒ–è¨­å®š
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
export CUDA_VISIBLE_DEVICES=0

# å®Ÿè¡Œ
python gemini_flash2_sample.py
```

**çµæœ**:
- âœ… å®Œå…¨å‹•ä½œ
- âš¡ Flash Attention 2.0 å¯¾å¿œï¼ˆé«˜é€ŸåŒ–ï¼‰
- ğŸ§  Gemini Flash 2.0 å¯¾å¿œ

## âš¡ æ‰‹å‹•Flash Attentionã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

### æ¨™æº–ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
```bash
pip install flash-attn --no-build-isolation
```

### ãƒ—ãƒªã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ç‰ˆï¼ˆæ¨å¥¨ï¼‰
```bash
# ç‰¹å®šãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®å ´åˆ
pip install https://github.com/Dao-AILab/flash-attention/releases/download/v2.8.1/flash_attn-2.8.1+cu121torch2.2cxx11abiTRUE-cp311-cp311-linux_x86_64.whl
```

### ã‚½ãƒ¼ã‚¹ã‹ã‚‰ãƒ“ãƒ«ãƒ‰
```bash
git clone https://github.com/Dao-AILab/flash-attention.git
cd flash-attention
pip install .
```

## ğŸ”„ ç’°å¢ƒè¨­å®š

### 1. ç’°å¢ƒå¤‰æ•°ã®è¨­å®š

#### .envãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆï¼ˆæ¨å¥¨ï¼‰
```bash
# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
echo "GEMINI_API_KEY=your-actual-api-key-here" > .env

# ã¾ãŸã¯
cat > .env << 'EOF'
GEMINI_API_KEY=your-actual-api-key-here
EOF
```

**âš ï¸ é‡è¦**: 
- `.env`ãƒ•ã‚¡ã‚¤ãƒ«ã¯æ—¢ã«`.gitignore`ã«å«ã¾ã‚Œã¦ã„ã‚‹ãŸã‚ã€Gitãƒªãƒã‚¸ãƒˆãƒªã«ã‚³ãƒŸãƒƒãƒˆã•ã‚Œã¾ã›ã‚“
- å®Ÿéš›ã®API Keyã‚’çµ¶å¯¾ã«å…¬é–‹ãƒªãƒã‚¸ãƒˆãƒªã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãªã„ã§ãã ã•ã„
- ãƒãƒ¼ãƒ é–‹ç™ºã§ã¯`.env.example`ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¦ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’å…±æœ‰ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨

#### ç’°å¢ƒå¤‰æ•°ã®ç›´æ¥è¨­å®š
```bash
# Gemini API Key ã‚’è¨­å®š
export GEMINI_API_KEY="your-actual-api-key-here"

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿ï¼ˆæ¨å¥¨ï¼‰
source .env

# GPUæœ€é©åŒ–è¨­å®šï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
export CUDA_VISIBLE_DEVICES=0
```

### 2. ä¾å­˜é–¢ä¿‚ã®æ§‹æˆ
```bash
# å…¨ä¾å­˜é–¢ä¿‚ï¼ˆæ¨å¥¨ï¼‰
pip install torch>=2.2.0
pip install sentence-transformers>=3.0.0
pip install google-generativeai>=0.3.0
pip install scikit-learn>=1.3.0
pip install fastapi>=0.115.0
pip install uvicorn>=0.30.0
pip install pydantic>=2.10.0
pip install pytz>=2024.1

# GPUé«˜é€ŸåŒ–ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
pip install flash-attn>=2.8.1
```

## ğŸŒ APIä½¿ç”¨æ–¹æ³•

### ã‚µãƒ¼ãƒãƒ¼èµ·å‹•å¾Œã®ã‚¢ã‚¯ã‚»ã‚¹

- **API ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc
- **ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯**: http://localhost:8000/health

### ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢API

#### ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
```
POST /search
```

#### ãƒªã‚¯ã‚¨ã‚¹ãƒˆå½¢å¼
```json
{
  "query": "AIæŠ€è¡“ã«ã¤ã„ã¦",
  "limit": 10,
  "use_flash_attention": true,
  "use_gemini_enhancement": true
}
```

#### ãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼
```json
{
  "results": [
    {
      "id": 1,
      "content": "ä»Šæ—¥ã¯æ–°ã—ã„AIæŠ€è¡“ã«ã¤ã„ã¦å­¦ã‚“ã ã€‚ç‰¹ã«Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®æ³¨æ„æ©Ÿæ§‹ãŒèˆˆå‘³æ·±ã‹ã£ãŸã€‚",
      "importance": 8,
      "similarity_score": 0.95,
      "last_accessed": "2024-01-15T10:30:00+09:00",
      "created_at": "2024-01-15T10:30:00+09:00"
    }
  ],
  "query_enhanced": "AIæŠ€è¡“ äººå·¥çŸ¥èƒ½ æ©Ÿæ¢°å­¦ç¿’ æ·±å±¤å­¦ç¿’ ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯",
  "processing_time": 0.12,
  "model_info": {
    "embedding_model": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
    "device": "cuda",
    "flash_attention_enabled": true,
    "gemini_enhancement_enabled": true,
    "cuda_available": true
  }
}
```

## ğŸ” æ©Ÿèƒ½è©³ç´°

### 1. ã‚¯ã‚¨ãƒªæ‹¡å¼µæ©Ÿèƒ½
Gemini Flash 2.0ãŒã‚¯ã‚¨ãƒªã‚’è‡ªå‹•çš„ã«æ‹¡å¼µãƒ»æ”¹å–„ã—ã¾ã™ï¼š

```python
# å…ƒã®ã‚¯ã‚¨ãƒª
"AIæŠ€è¡“ã«ã¤ã„ã¦"

# æ‹¡å¼µã•ã‚ŒãŸã‚¯ã‚¨ãƒª
"AIæŠ€è¡“ äººå·¥çŸ¥èƒ½ æ©Ÿæ¢°å­¦ç¿’ æ·±å±¤å­¦ç¿’ ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ Transformer ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£"
```

### 2. Flash Attention 2.0 é«˜é€ŸåŒ–
å¾“æ¥ã®ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã¨æ¯”è¼ƒã—ã¦å¤§å¹…ãªé«˜é€ŸåŒ–ã‚’å®Ÿç¾ï¼š

- **CPUå‡¦ç†**: é€šå¸¸ã®ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—
- **GPUå‡¦ç†**: Flash Attention 2.0ã«ã‚ˆã‚‹é«˜é€Ÿè¨ˆç®—
- **ãƒ¡ãƒ¢ãƒªåŠ¹ç‡**: å¤§å¹…ãªãƒ¡ãƒ¢ãƒªå‰Šæ¸›

### 3. å¤šè¨€èªå¯¾å¿œ
50è¨€èªã«å¯¾å¿œã—ãŸã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ï¼š

```python
# æ—¥æœ¬èª
"ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®é€²æ—ã«ã¤ã„ã¦"

# è‹±èª
"About project progress"

# ä¸­å›½èª
"å…³äºé¡¹ç›®è¿›å±•"
```

## ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ

### å‡¦ç†æ™‚é–“ã®æ¯”è¼ƒ
- **é€šå¸¸ã®ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦**: 0.5-1.0ç§’
- **Flash Attention 2.0**: 0.1-0.3ç§’
- **é€Ÿåº¦å‘ä¸Š**: 3-5å€é«˜é€Ÿ

### ç’°å¢ƒåˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹

| ç’°å¢ƒ | Flash Attention | å‡¦ç†é€Ÿåº¦ | ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ | æ¨å¥¨åº¦ |
|------|-----------------|----------|--------------|--------|
| macOS (M1/M2/M3) | âŒ ãªã— | æ¨™æº– | æ¨™æº– | âœ… é–‹ç™ºç”¨ |
| Windows + GPU | âš ï¸ æ‰‹å‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« | 1.5-2x | 0.7x | âš ï¸ å®Ÿé¨“çš„ |
| Linux + NVIDIA | âœ… å®Œå…¨å¯¾å¿œ | 3-5x | 0.3x | ğŸš€ æœ¬ç•ªæ¨å¥¨ |

## ğŸ› ï¸ ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º

### 1. åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®å¤‰æ›´
```python
# ä»–ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
EMBEDDING_MODEL = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
```

### 2. Geminiãƒ¢ãƒ‡ãƒ«ã®å¤‰æ›´
```python
# ä»–ã®Geminiãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
self.gemini_model = genai.GenerativeModel('gemini-pro')
```

### 3. Flash Attentionãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®èª¿æ•´
```python
# Softmax scalingèª¿æ•´
attention_output = flash_attn_func(q, k, v, softmax_scale=0.5)
```

## ğŸ§ª å‹•ä½œç¢ºèªãƒ»ãƒ†ã‚¹ãƒˆ

### åŸºæœ¬å‹•ä½œç¢ºèª
```bash
python -c "
import torch
import sentence_transformers
import google.generativeai
print('âœ… åŸºæœ¬ä¾å­˜é–¢ä¿‚: OK')
"
```

### Flash Attentionç¢ºèª
```bash
python -c "
try:
    from flash_attn import flash_attn_func
    print('âœ… Flash Attention 2.0: åˆ©ç”¨å¯èƒ½')
except ImportError:
    print('âš ï¸ Flash Attention 2.0: åˆ©ç”¨ä¸å¯ï¼ˆé€šå¸¸å‹•ä½œï¼‰')
"
```

### CUDAç¢ºèª
```bash
python -c "
import torch
print('CUDAåˆ©ç”¨å¯èƒ½:', torch.cuda.is_available())
if torch.cuda.is_available():
    print('GPU:', torch.cuda.get_device_name(0))
    print('CUDA version:', torch.version.cuda)
"
```

### ç’°å¢ƒç¢ºèª
```bash
# CUDAç¢ºèª
nvidia-smi

# Pythonç’°å¢ƒç¢ºèª
python -c "
import torch
print('CUDAåˆ©ç”¨å¯èƒ½:', torch.cuda.is_available())
print('GPUæ•°:', torch.cuda.device_count())
if torch.cuda.is_available():
    print('GPUå:', torch.cuda.get_device_name(0))
"
```

### API ãƒ†ã‚¹ãƒˆ
```bash
# ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
curl http://localhost:8000/health

# æ¤œç´¢ãƒ†ã‚¹ãƒˆ
curl -X POST "http://localhost:8000/search" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "AIæŠ€è¡“ã«ã¤ã„ã¦",
    "limit": 5,
    "use_flash_attention": true,
    "use_gemini_enhancement": true
  }'
```

## âš ï¸ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### 1. Flash Attention ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¤±æ•—
```bash
# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢
pip cache purge

# æ‰‹å‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install flash-attn --no-build-isolation --no-cache-dir

# åˆ¥ã®CUDAãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’è©¦ã™
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

### 2. PyTorch CUDAèªè­˜ã—ãªã„
```bash
# CUDAç‰ˆPyTorchå†ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip uninstall torch torchvision torchaudio
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

### 3. Gemini API ã‚¨ãƒ©ãƒ¼
```bash
# .envãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª
cat .env
# ã¾ãŸã¯
ls -la .env

# API Keyç¢ºèªï¼ˆç’°å¢ƒå¤‰æ•°ï¼‰
echo $GEMINI_API_KEY

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
source .env && echo $GEMINI_API_KEY

# æ¨©é™ç¢ºèª
python -c "
import google.generativeai as genai
genai.configure(api_key='your-api-key')
model = genai.GenerativeModel('gemini-2.0-flash-exp')
print('Geminiæ¥ç¶šæˆåŠŸ')
"
```

### 4. ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚¨ãƒ©ãƒ¼
```bash
# ãƒãƒƒãƒã‚µã‚¤ã‚ºèª¿æ•´
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128

# GPUæœ€é©åŒ–è¨­å®š
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
```

### 5. æ¨©é™ã‚¨ãƒ©ãƒ¼ï¼ˆLinuxï¼‰
```bash
# ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ­ãƒ¼ã‚«ãƒ«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install --user -r requirements.txt
```

## ğŸ“ é–‹ç™ºãƒãƒ¼ãƒˆ

### è‡ªå‹•ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½
- Flash Attention 2.0ãŒåˆ©ç”¨ã§ããªã„å ´åˆã€è‡ªå‹•çš„ã«ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
- Gemini APIãŒåˆ©ç”¨ã§ããªã„å ´åˆã€ã‚¯ã‚¨ãƒªæ‹¡å¼µãªã—ã§å‹•ä½œ
- ç’°å¢ƒã«å¿œã˜ã¦æœ€é©ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’è‡ªå‹•é¸æŠ

### ä¾å­˜é–¢ä¿‚ç®¡ç†
- `requirements.txt`: åŸºæœ¬å‹•ä½œã«å¿…è¦ãªä¾å­˜é–¢ä¿‚ã®ã¿
- Flash Attention: ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆï¼ˆç’°å¢ƒã«å¿œã˜ã¦æ‰‹å‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼‰
- å…¨ç’°å¢ƒï¼ˆmacOS/Windows/Linuxï¼‰å¯¾å¿œ

**æ¨å¥¨**: é–‹ç™ºã¯ä»»æ„ã®ç’°å¢ƒã€æœ¬ç•ªã¯**Linux + NVIDIA GPUç’°å¢ƒ**ã§æœ€é«˜æ€§èƒ½ã‚’å®Ÿç¾ã€‚ 
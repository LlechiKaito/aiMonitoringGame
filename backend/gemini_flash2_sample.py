#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Gemini Flash 2.0 + Flash Attention 2.0 çµ±åˆã‚µãƒ³ãƒ—ãƒ«
AI Monitoring Game Backendç”¨ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢æ©Ÿèƒ½
"""

import os
import sys
import json
import asyncio
from datetime import datetime
from typing import List, Dict, Any, Optional
import logging

# ä¾å­˜é–¢ä¿‚ã®ç¢ºèªã¨ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
try:
    import torch
    from sentence_transformers import SentenceTransformer
    import google.generativeai as genai
    import numpy as np
    from sklearn.metrics.pairwise import cosine_similarity
    import uvicorn
    from fastapi import FastAPI, HTTPException
    from pydantic import BaseModel
    import pytz
except ImportError as e:
    print(f"å¿…è¦ãªä¾å­˜é–¢ä¿‚ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“: {e}")
    print("ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§ä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„:")
    print("pip install torch sentence-transformers google-generativeai scikit-learn fastapi uvicorn pydantic pytz")
    sys.exit(1)

# Flash Attentionã®ç¢ºèªï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
try:
    from flash_attn import flash_attn_func
    FLASH_ATTENTION_AVAILABLE = True
    print("âœ… Flash Attention 2.0 ãŒåˆ©ç”¨å¯èƒ½ã§ã™")
except ImportError:
    FLASH_ATTENTION_AVAILABLE = False
    print("âš ï¸  Flash Attention 2.0 ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚é€šå¸¸ã®ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
    flash_attn_func = None

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# è¨­å®š
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "your-api-key-here")
EMBEDDING_MODEL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
JST = pytz.timezone('Asia/Tokyo')

class SemanticSearchRequest(BaseModel):
    query: str
    limit: int = 10
    use_flash_attention: bool = True
    use_gemini_enhancement: bool = True

class SemanticSearchResponse(BaseModel):
    results: List[Dict[str, Any]]
    query_enhanced: Optional[str] = None
    processing_time: float
    model_info: Dict[str, Any]

class GeminiFlash2SemanticSearch:
    """
    Gemini Flash 2.0 ã¨ Flash Attention 2.0 ã‚’çµ±åˆã—ãŸã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢
    """
    
    def __init__(self):
        self.setup_gemini()
        self.setup_embeddings()
        self.setup_flash_attention()
        self.sample_memories = self.create_sample_data()
        
    def setup_gemini(self):
        """Gemini Flash 2.0ã®è¨­å®š"""
        if GEMINI_API_KEY == "your-api-key-here":
            logger.warning("Gemini API KeyãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ç’°å¢ƒå¤‰æ•°GEMINI_API_KEYã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
            self.gemini_available = False
        else:
            try:
                genai.configure(api_key=GEMINI_API_KEY)
                self.gemini_model = genai.GenerativeModel('gemini-2.0-flash-exp')
                self.gemini_available = True
                logger.info("âœ… Gemini Flash 2.0 åˆæœŸåŒ–æˆåŠŸ")
            except Exception as e:
                logger.error(f"âŒ Gemini Flash 2.0 åˆæœŸåŒ–å¤±æ•—: {e}")
                self.gemini_available = False
    
    def setup_embeddings(self):
        """åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®è¨­å®š"""
        try:
            self.embedding_model = SentenceTransformer(EMBEDDING_MODEL)
            self.embedding_model.eval()
            logger.info(f"âœ… åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ: {EMBEDDING_MODEL}")
        except Exception as e:
            logger.error(f"âŒ åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
            raise
    
    def setup_flash_attention(self):
        """Flash Attention 2.0ã®è¨­å®š"""
        if not FLASH_ATTENTION_AVAILABLE:
            self.flash_attention_available = False
            logger.warning("âš ï¸ Flash Attention 2.0 æœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã®ãŸã‚ç„¡åŠ¹åŒ–")
            return
            
        try:
            if torch.cuda.is_available():
                # Flash Attention ãƒ†ã‚¹ãƒˆ
                test_q = torch.randn(1, 10, 8, 64, dtype=torch.float16, device='cuda')
                test_k = torch.randn(1, 10, 8, 64, dtype=torch.float16, device='cuda')
                test_v = torch.randn(1, 10, 8, 64, dtype=torch.float16, device='cuda')
                
                with torch.no_grad():
                    _ = flash_attn_func(test_q, test_k, test_v)
                
                self.flash_attention_available = True
                logger.info("âœ… Flash Attention 2.0 åˆæœŸåŒ–æˆåŠŸ")
            else:
                self.flash_attention_available = False
                logger.warning("âš ï¸ CUDAæœªå¯¾å¿œã®ãŸã‚ Flash Attention 2.0 ã‚’ç„¡åŠ¹åŒ–")
        except Exception as e:
            logger.error(f"âŒ Flash Attention 2.0 åˆæœŸåŒ–å¤±æ•—: {e}")
            self.flash_attention_available = False
    
    def create_sample_data(self) -> List[Dict[str, Any]]:
        """ã‚µãƒ³ãƒ—ãƒ«ãƒ¡ãƒ¢ãƒªãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ"""
        return [
            {
                "id": 1,
                "content": "ä»Šæ—¥ã¯æ–°ã—ã„AIæŠ€è¡“ã«ã¤ã„ã¦å­¦ã‚“ã ã€‚ç‰¹ã«Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®æ³¨æ„æ©Ÿæ§‹ãŒèˆˆå‘³æ·±ã‹ã£ãŸã€‚",
                "importance": 8,
                "last_accessed": datetime.now(JST),
                "created_at": datetime.now(JST)
            },
            {
                "id": 2,
                "content": "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ç· åˆ‡ãŒè¿‘ã¥ã„ã¦ã„ã‚‹ã€‚ã‚¿ã‚¹ã‚¯ã®å„ªå…ˆé †ä½ã‚’è¦‹ç›´ã™å¿…è¦ãŒã‚ã‚‹ã€‚",
                "importance": 9,
                "last_accessed": datetime.now(JST),
                "created_at": datetime.now(JST)
            },
            {
                "id": 3,
                "content": "ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã®å®Ÿè£…ã§Flash Attentionã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§å¤§å¹…ãªé«˜é€ŸåŒ–ãŒæœŸå¾…ã§ãã‚‹ã€‚",
                "importance": 7,
                "last_accessed": datetime.now(JST),
                "created_at": datetime.now(JST)
            },
            {
                "id": 4,
                "content": "ãƒãƒ¼ãƒ ãƒ¡ãƒ³ãƒãƒ¼ã¨ã®ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ”¹å–„ã™ã‚‹ãŸã‚ã®ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’è€ƒãˆãŸã€‚",
                "importance": 6,
                "last_accessed": datetime.now(JST),
                "created_at": datetime.now(JST)
            },
            {
                "id": 5,
                "content": "Gemini Flash 2.0ã®æ–°æ©Ÿèƒ½ã‚’ä½¿ã£ã¦ã€ã‚ˆã‚Šè‡ªç„¶ãªå¯¾è©±ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã—ãŸã„ã€‚",
                "importance": 8,
                "last_accessed": datetime.now(JST),
                "created_at": datetime.now(JST)
            }
        ]
    
    async def enhance_query_with_gemini(self, query: str) -> str:
        """Gemini Flash 2.0ã‚’ä½¿ç”¨ã—ãŸã‚¯ã‚¨ãƒªæ‹¡å¼µ"""
        if not self.gemini_available:
            return query
        
        try:
            prompt = f"""
            ä»¥ä¸‹ã®æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ã€ã‚ˆã‚ŠåŠ¹æœçš„ãªã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã®ãŸã‚ã«æ‹¡å¼µãƒ»æ”¹å–„ã—ã¦ãã ã•ã„ã€‚
            é–¢é€£ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚„åŒç¾©èªã‚’å«ã‚ã¦ã€æ¤œç´¢ç²¾åº¦ã‚’å‘ä¸Šã•ã›ã¦ãã ã•ã„ã€‚
            
            å…ƒã®ã‚¯ã‚¨ãƒª: {query}
            
            æ‹¡å¼µã•ã‚ŒãŸã‚¯ã‚¨ãƒªã‚’æ—¥æœ¬èªã§è¿”ã—ã¦ãã ã•ã„ã€‚
            """
            
            response = await asyncio.to_thread(
                self.gemini_model.generate_content, prompt
            )
            
            enhanced_query = response.text.strip()
            logger.info(f"ã‚¯ã‚¨ãƒªæ‹¡å¼µ: '{query}' -> '{enhanced_query}'")
            return enhanced_query
            
        except Exception as e:
            logger.error(f"Geminiã‚¯ã‚¨ãƒªæ‹¡å¼µã‚¨ãƒ©ãƒ¼: {e}")
            return query
    
    def compute_embeddings(self, texts: List[str]) -> np.ndarray:
        """ãƒ†ã‚­ã‚¹ãƒˆã®åŸ‹ã‚è¾¼ã¿è¨ˆç®—"""
        try:
            with torch.no_grad():
                embeddings = self.embedding_model.encode(texts)
            return embeddings
        except Exception as e:
            logger.error(f"åŸ‹ã‚è¾¼ã¿è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def compute_similarity_with_flash_attention(self, 
                                              query_embedding: np.ndarray, 
                                              memory_embeddings: np.ndarray) -> np.ndarray:
        """Flash Attention 2.0ã‚’ä½¿ç”¨ã—ãŸé¡ä¼¼åº¦è¨ˆç®—"""
        if not self.flash_attention_available or not FLASH_ATTENTION_AVAILABLE:
            return cosine_similarity([query_embedding], memory_embeddings)[0]
        
        try:
            # åŸ‹ã‚è¾¼ã¿ã‚’Flash Attentionå½¢å¼ã«å¤‰æ›
            q = torch.tensor(query_embedding, dtype=torch.float16, device='cuda').unsqueeze(0).unsqueeze(0).unsqueeze(0)
            k = torch.tensor(memory_embeddings, dtype=torch.float16, device='cuda').unsqueeze(0).unsqueeze(0)
            v = k.clone()
            
            # Flash Attentionå®Ÿè¡Œ
            with torch.no_grad():
                attention_output = flash_attn_func(q, k, v, softmax_scale=1.0/np.sqrt(query_embedding.shape[0]))
            
            # é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢ã®è¨ˆç®—
            similarities = attention_output.squeeze().cpu().numpy()
            
            # æ­£è¦åŒ–
            similarities = (similarities - similarities.min()) / (similarities.max() - similarities.min())
            
            return similarities
            
        except Exception as e:
            logger.error(f"Flash Attentioné¡ä¼¼åº¦è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦é€šå¸¸ã®ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’ä½¿ç”¨
            return cosine_similarity([query_embedding], memory_embeddings)[0]
    
    async def search(self, request: SemanticSearchRequest) -> SemanticSearchResponse:
        """ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã®å®Ÿè¡Œ"""
        start_time = datetime.now()
        
        # ã‚¯ã‚¨ãƒªã®æ‹¡å¼µ
        enhanced_query = request.query
        if request.use_gemini_enhancement:
            enhanced_query = await self.enhance_query_with_gemini(request.query)
        
        # åŸ‹ã‚è¾¼ã¿ã®è¨ˆç®—
        query_embedding = self.compute_embeddings([enhanced_query])[0]
        memory_texts = [memory["content"] for memory in self.sample_memories]
        memory_embeddings = self.compute_embeddings(memory_texts)
        
        # é¡ä¼¼åº¦è¨ˆç®—
        if request.use_flash_attention:
            similarities = self.compute_similarity_with_flash_attention(query_embedding, memory_embeddings)
        else:
            similarities = cosine_similarity([query_embedding], memory_embeddings)[0]
        
        # çµæœã®æ•´ç†
        results = []
        for i, similarity in enumerate(similarities):
            memory = self.sample_memories[i].copy()
            memory["similarity_score"] = float(similarity)
            memory["last_accessed"] = memory["last_accessed"].isoformat()
            memory["created_at"] = memory["created_at"].isoformat()
            results.append(memory)
        
        # é¡ä¼¼åº¦é †ã§ã‚½ãƒ¼ãƒˆ
        results.sort(key=lambda x: x["similarity_score"], reverse=True)
        results = results[:request.limit]
        
        # å‡¦ç†æ™‚é–“ã®è¨ˆç®—
        processing_time = (datetime.now() - start_time).total_seconds()
        
        # ãƒ¢ãƒ‡ãƒ«æƒ…å ±
        model_info = {
            "embedding_model": EMBEDDING_MODEL,
            "device": DEVICE,
            "flash_attention_enabled": request.use_flash_attention and self.flash_attention_available,
            "gemini_enhancement_enabled": request.use_gemini_enhancement and self.gemini_available,
            "cuda_available": torch.cuda.is_available()
        }
        
        return SemanticSearchResponse(
            results=results,
            query_enhanced=enhanced_query if enhanced_query != request.query else None,
            processing_time=processing_time,
            model_info=model_info
        )

# FastAPIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
app = FastAPI(
    title="Gemini Flash 2.0 + Flash Attention 2.0 ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢",
    description="AI Monitoring Game Backendç”¨ã®é«˜é€Ÿã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢API",
    version="1.0.0"
)

# ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
semantic_search = GeminiFlash2SemanticSearch()

@app.post("/search", response_model=SemanticSearchResponse)
async def search_memories(request: SemanticSearchRequest):
    """
    ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢API
    """
    try:
        return await semantic_search.search(request)
    except Exception as e:
        logger.error(f"æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health_check():
    """
    ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯API
    """
    return {
        "status": "healthy",
        "timestamp": datetime.now(JST).isoformat(),
        "cuda_available": torch.cuda.is_available(),
        "flash_attention_available": semantic_search.flash_attention_available,
        "gemini_available": semantic_search.gemini_available
    }

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸš€ Gemini Flash 2.0 + Flash Attention 2.0 ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ èµ·å‹•ä¸­...")
    print("ğŸ“– API ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: http://localhost:8000/docs")
    print("ğŸ” ReDoc: http://localhost:8000/redoc")
    print("ğŸ’» CUDAåˆ©ç”¨å¯èƒ½:", torch.cuda.is_available())
    print("âš¡ Flash Attention 2.0 å¯¾å¿œ:", semantic_search.flash_attention_available)
    print("ğŸ§  Gemini Flash 2.0 å¯¾å¿œ:", semantic_search.gemini_available)
    print("â¹ï¸  åœæ­¢ã™ã‚‹ã«ã¯ Ctrl+C ã‚’æŠ¼ã—ã¦ãã ã•ã„")
    print("-" * 50)
    
    # ã‚µãƒ¼ãƒãƒ¼èµ·å‹•
    uvicorn.run(app, host="0.0.0.0", port=8000, reload=False)

if __name__ == "__main__":
    main() 